### Analysis instructions for running the reputation simulation
   (1) Modify json config file for your run
  * "noUnratednoDenomConserv5SAP182.json" is an example of a config file
  * Config files have four sections: macro_views, test, batch, and parameters. 
    *  Marcroviews let the user use equations to parameterize the simulation in automated fashion:  parameters entered here are transalated with an adapter. 
    *  Test tells what unittests and ranges apply to this scenario.
    *  Batch tells what parameters in the parameter section to run a cartesian product on, and what to call the outfiles.
    *  Parameters contains all the parameters of a single run
 * All the parameters you want to modify for a simple run are in the parameters section
   * The switch that tells if you you are running java or python is at ['parameters']['use_java']
   * The name of the output directory is at ['parameters']['output_path']
   * The port you are running on is at ['parameters']['port']
   * The random number seed is at ['parameters']['seed']
   * All the reputation system parameters are at ['parameters']['reputation_parameters']
   
  (2) Enter the installation environment and run the simulation as described under "usage" above
  * If the output directory already exists it will be overwritten.

 (3)  Output files are in the output path directory and are named with the parameters in the configfile batch section
  * params file is a copy of the config file generated by the batch for each run
  * transactions files have all the current transactions
  * boolean files tell if an agent is good or bad
  * user files tell the how good the agent is
  * market volume report reports on market volumes of trade between types of agents
  * rank history prints out the ranks of all agents in each row, with -1 meaning no ranking
		
 (4)  To get metrics on the run, run unittests , which here are soft unittests for analysis purposes
  * Make sure the config file mentioned in the Tests of the test folder contains the tests that you want run on each parameter combination and the directory of the simulation run to be tested.
    * Modify the test.json file to have the correct tests and correct directory, the rest of the file is ignored.  You can do this by copying the config file onto the file "test.json"
    * Alternatively, if needed, you can change the config file name in every test in the reputation/test folder to the current config file
    
    ```sed -i 's/test.json/noUnratednoDenomConserv5SAP182.json/g' *Tests.py```
  * Run the unittests, pointing to the test directory 
    
    ```python -m unittest discover -s /home/reputation/snsim/reputation/test -t /home/reputation/snsim/reputation/test -p *Tests*```
  * Metric results will appear in the output file
    * error_log.txt contains all the metrics, in appended timestamp mode to record metric changes over time
    * For worksheet analysis, individual test results appear in tsv files
      * These tsv files can be read into a jupyter notebook, that generates a composite tsv for analysis in a worksheet
      * reputation_results-conserv_new_unrestricted_mem.ipynb is one example
			